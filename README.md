# üìâ Benchmark des M√©thodes de Descente de gradient

## Introduction
TP de recherche et d'analyse sur les m√©thodes de descente de gradient en optimisation convexe. Ce travail a √©t√© r√©alis√© dans le cadre du cours d'optimisation convexe √† EPITA par Marc Lagoin, Daniel Rosa et Th√©o Bonzi.

## Date
14 juin 2023

## Table des Mati√®res
1. **Introduction**
2. **D√©coupage des T√¢ches**
3. **Benchmark 1: Influence du Pas sur le Nombre d'It√©rations**
4. **Benchmark 2: Optimisation des Hyperparam√®tres pour le Crit√®re d'Armijo**
5. **Benchmark 3: Crit√®re d'Arr√™t vs Nombre de It√©rations**

## Objectifs
Le projet OCVX vise √† :
- Explorer l'impact du pas de descente sur le nombre d'it√©rations n√©cessaires pour la convergence.
- √âtudier l'influence des hyperparam√®tres alpha et beta dans le crit√®re d'Armijo sur la performance de l'optimisation.
- Examiner l'effet du crit√®re d'arr√™t sur le nombre d'it√©rations de la m√©thode de descente.
