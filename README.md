# OCVX - Benchmark des Méthodes de Descente de gradient

## Introduction
OCVX est un TP de recherche et d'analyse sur les méthodes de descente de gradient en optimisation convexe. Ce travail a été réalisé dans le cadre du cours d'optimisation convexe à EPITA par Théo Bonzi, Marc Lagoin et Daniel Rosa.

## Date
14 juin 2023

## Table des Matières
1. **Introduction**
2. **Découpage des Tâches**
3. **Benchmark 1: Influence du Pas sur le Nombre d'Itérations**
4. **Benchmark 2: Optimisation des Hyperparamètres pour le Critère d'Armijo**
5. **Benchmark 3: Critère d'Arrêt vs Nombre de Itérations**

## Objectifs
Le projet OCVX vise à :
- Explorer l'impact du pas de descente sur le nombre d'itérations nécessaires pour la convergence.
- Étudier l'influence des hyperparamètres alpha et beta dans le critère d'Armijo sur la performance de l'optimisation.
- Examiner l'effet du critère d'arrêt sur le nombre d'itérations de la méthode de descente.
